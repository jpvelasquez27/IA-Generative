ğŸ§  Proyecto Chatbot IA con LLaMA + FastAPI + Chroma + Streamlit + Docker
ğŸ“‹ DescripciÃ³n general

Este proyecto implementa un chatbot con inteligencia artificial basado en modelos LLaMA ejecutados localmente, con capacidades de bÃºsqueda contextual mediante una base de datos vectorial (Chroma) y una API desarrollada con FastAPI.
AdemÃ¡s, incluye una interfaz visual en Streamlit para probar el sistema fÃ¡cilmente.

La arquitectura sigue el patrÃ³n RAG (Retrieval-Augmented Generation):

Se ingestan documentos o textos.

Se convierten en embeddings vectoriales.

Se almacenan en la base vectorial.

El modelo LLaMA genera respuestas contextualizadas.

ğŸ§° TecnologÃ­as utilizadas
Componente	TecnologÃ­a	DescripciÃ³n
ğŸ§  Modelo IA	LLaMA / llama-cpp-python
	Modelo de lenguaje ejecutado localmente.
ğŸ” Base Vectorial	ChromaDB
	Almacenamiento y bÃºsqueda semÃ¡ntica de embeddings.
ğŸ§© Embeddings	Sentence-Transformers
	Convierte texto a vectores numÃ©ricos.
âš™ï¸ Backend	FastAPI
	API moderna y rÃ¡pida en Python.
ğŸ’» Interfaz	Streamlit
	Panel web para visualizar y probar el chatbot.
ğŸ³ Contenedores	Docker
	Aislamiento y despliegue consistente del proyecto.

ğŸ§± Estructura del proyecto
chatbot-llama-fastapi/
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ docker-compose.yml
â”œâ”€ Dockerfile.api
â”œâ”€ Dockerfile.ui
â”œâ”€ .env
â”œâ”€ models/
â”‚   â””â”€ your-model.bin
â”œâ”€ src/
â”‚  â”œâ”€ app/
â”‚  â”‚  â”œâ”€ main.py
â”‚  â”‚  â””â”€ llm_client.py
â”‚  â”œâ”€ utils/
â”‚  â”‚  â”œâ”€ embeddings.py
â”‚  â””â”€ vector_db/
â”‚     â””â”€ chroma_client.py
â””â”€ frontend/
   â””â”€ streamlit_app.py

âš™ï¸ Variables de entorno (.env)

Crea un archivo .env en la raÃ­z con:

MODEL_PATH=/app/models/your-model.bin
CHROMA_DB_PATH=/app/chroma_db
FASTAPI_PORT=8000
STREAMLIT_PORT=8501

ğŸ³ ConfiguraciÃ³n de Docker
Dockerfile.api
# Imagen para el backend (FastAPI)
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY ./src ./src
COPY ./models ./models
COPY .env .

EXPOSE 8000

CMD ["uvicorn", "src.app.main:app", "--host", "0.0.0.0", "--port", "8000"]

Dockerfile.ui
# Imagen para la interfaz Streamlit
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY ./frontend ./frontend
COPY .env .

EXPOSE 8501

CMD ["streamlit", "run", "frontend/streamlit_app.py", "--server.port=8501", "--server.address=0.0.0.0"]

docker-compose.yml
version: "3.9"

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: chatbot_api
    env_file: .env
    volumes:
      - ./models:/app/models
      - ./src:/app/src
      - ./chroma_db:/app/chroma_db
    ports:
      - "${FASTAPI_PORT}:8000"
    restart: always

  ui:
    build:
      context: .
      dockerfile: Dockerfile.ui
    container_name: chatbot_ui
    env_file: .env
    depends_on:
      - api
    ports:
      - "${STREAMLIT_PORT}:8501"
    restart: always

ğŸš€ EjecuciÃ³n con Docker
1ï¸âƒ£ Construir e iniciar los contenedores
docker compose up --build


Esto levantarÃ¡:

Servicio	URL	DescripciÃ³n
ğŸ§© API FastAPI	http://localhost:8000/docs
	Endpoints para ingestiÃ³n y consulta.
ğŸ’» Interfaz Streamlit	http://localhost:8501
	Interfaz web del chatbot.
2ï¸âƒ£ Apagar los contenedores
docker compose down

3ï¸âƒ£ Ver logs en tiempo real
docker compose logs -f

ğŸ“¦ InstalaciÃ³n sin Docker (modo manual)

Si prefieres probar el proyecto sin contenedores:

python -m venv .venv
source .venv/bin/activate  # o .venv\Scripts\activate en Windows
pip install -r requirements.txt
uvicorn src.app.main:app --reload


Luego, en otra terminal:

streamlit run frontend/streamlit_app.py

ğŸ§© Endpoints principales
POST /ingest

Ingesta textos o documentos en la base vectorial.

{
  "texts": ["SAP ABAP es un lenguaje de programaciÃ³n empresarial."]
}

POST /query

Consulta al chatbot con recuperaciÃ³n contextual.

{
  "query": "Â¿QuÃ© es SAP ABAP?",
  "top_k": 3
}

ğŸ’¡ Recomendaciones

Usa modelos quantizados (Q4/Q5) para menor consumo de RAM.

En producciÃ³n, considera usar Qdrant o Milvus como vector DB externa.

Si vas a correr en servidor con GPU, prueba vLLM como alternativa mÃ¡s eficiente al ejecutar modelos grandes.


Autor
Proyecto creado por Juan Pablo VelÃ¡squez
